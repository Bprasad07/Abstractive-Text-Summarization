{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pjyY_EWJhLM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras_nlp\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from keras_nlp.layers import TransformerEncoder\n",
    "\n",
    "# Load the CNN/Daily Mail dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/GWAR/train.csv\")\n",
    "test_df = pd.read_csv(\"/content/drive/MyDrive/GWAR/test.csv\")\n",
    "validation_df = pd.read_csv(\"/content/drive/MyDrive/GWAR/validation.csv\")\n",
    "\n",
    "df = df.iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWZCqo2qsiFf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNwnkg3lxz6Z"
   },
   "outputs": [],
   "source": [
    "# Define the tokenizer and prepare the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['article'].values.tolist() + df['highlights'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eMBC01mvprM"
   },
   "outputs": [],
   "source": [
    "# Define the target vocabulary size\n",
    "target_vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgGDfjDfyki6"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = tokenizer.texts_to_sequences(df['article'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Jkndq-eymbd"
   },
   "outputs": [],
   "source": [
    "decoder_input_data = tokenizer.texts_to_sequences(df['highlights'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgx7e0XmyTH8"
   },
   "outputs": [],
   "source": [
    "decoder_target_data = [sequence[1:] for sequence in decoder_input_data]\n",
    "decoder_target_data.append([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zvc86iuLyYcp"
   },
   "outputs": [],
   "source": [
    "MAX_LEN=1000\n",
    "encoder_input_data = pad_sequences(encoder_input_data, maxlen=MAX_LEN, padding='post')\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=MAX_LEN, padding='post')\n",
    "decoder_target_data = pad_sequences(decoder_target_data, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqnCIqm4zwSl"
   },
   "outputs": [],
   "source": [
    "decoder_target_data = (sequence[1:] for sequence in decoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGmKDHF5ROnu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=\"random_normal\", trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=\"random_normal\", trainable=True)\n",
    "        self.V = self.add_weight(shape=(input_shape[-1], 1), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = inputs[0]\n",
    "        k = inputs[1]\n",
    "        v = inputs[2]\n",
    "\n",
    "        score = tf.nn.tanh(tf.matmul(q, self.W1) + tf.matmul(k, self.W2))\n",
    "        score = tf.matmul(score, self.V)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        output = tf.reduce_sum(attention_weights * v, axis=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzxZ6o4T-eGa"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.additive_attention = AdditiveAttention()\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.feed_forward = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.additive_attention([inputs, inputs, inputs])\n",
    "        attn_output = self.dropout_1(attn_output, training=training)\n",
    "        out1 = self.layer_norm_1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.feed_forward(out1)\n",
    "        ffn_output = self.dropout_2(ffn_output, training=training)\n",
    "        return self.layer_norm_2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp1Q2hG2wmwS"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout_rate):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.additive_attention1 = AdditiveAttention()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.additive_attention2 = AdditiveAttention()\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # Multi-head attention block\n",
    "        attn1 = self.additive_attention1([x, x, x])\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layer_norm1(x + attn1)\n",
    "        \n",
    "        # Multi-head attention block with encoder output\n",
    "        attn2 = self.additive_attention2([enc_output, enc_output, out1])\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layer_norm2(out1 + attn2)\n",
    "        \n",
    "        # Feedforward block\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer_norm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Seo3A-ykwe-F"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, max_seq_len, d_model, n_heads, ff_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.encoder_blocks = [EncoderBlock(d_model, n_heads, ff_dim, dropout_rate) for _ in range(2)]\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        position = tf.cast(tf.range(position)[:, tf.newaxis], tf.float32)\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2) * -(tf.math.log(10000.0) / d_model))[:, tf.newaxis]\n",
    "        #div_term = tf.exp(tf.range(0, d_model, 2) * -(tf.math.log(10000.0) / d_model))\n",
    "        sin = tf.math.sin(position * div_term)\n",
    "        cos = tf.math.cos(position * div_term)\n",
    "        pos_encoding = tf.concat([sin, cos], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.Variable(pos_encoding, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVvlAEgN_lv0"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_vocab_size, max_seq_len, d_model=256, n_heads=8, ff_dim=512, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.decoder_blocks = [DecoderBlock(d_model, n_heads, ff_dim, dropout_rate) for _ in range(2)]\n",
    "        \n",
    "    def call(self, x, enc_output):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x, block_attention_weights = decoder_block(x, enc_output)\n",
    "            attention_weights[f'decoder_block{i+1}_block'] = block_attention_weights\n",
    "        \n",
    "        return x, attention_weights\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        position = tf.cast(tf.range(position)[:, tf.newaxis], tf.float32)\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2) * -(tf.math.log(10000.0) / d_model))\n",
    "        sin = tf.math.sin(position * div_term)\n",
    "        cos = tf.math.cos(position * div_term)\n",
    "        pos_encoding = tf.concat([sin, cos], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.Variable(pos_encoding, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_zVmVhh-jjH"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, max_seq_input, max_seq_target, d_model=256, n_heads=8, ff_dim=512, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_vocab_size, max_seq_input, d_model, n_heads, ff_dim, dropout_rate)\n",
    "        self.decoder = Decoder(target_vocab_size, max_seq_target, d_model, n_heads, ff_dim, dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input, target = inputs\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, target, maxlen=MAX_LEN)\n",
    "        \n",
    "        enc_output = self.encoder(input, enc_padding_mask)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(target, enc_output)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUG2hRpGurrd"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = tf.range(position, dtype=tf.float32)[:, tf.newaxis] / tf.pow(10000, tf.range(0, d_model, 2, dtype=tf.float32) / d_model)\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHbsmzstvujZ",
    "outputId": "676ce243-d65e-4e3f-be70-3818de2882e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adagrad.\n"
     ]
    }
   ],
   "source": [
    "# Define the model and compile it\n",
    "max_seq_input = max([len(txt) for txt in encoder_input_data])\n",
    "max_seq_target = max([len(txt) for txt in decoder_input_data])\n",
    "\n",
    "model = Transformer(\n",
    "    input_vocab_size=len(tokenizer.word_index) + 1,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_seq_input=max_seq_input,\n",
    "    max_seq_target=max_seq_target\n",
    ")\n",
    "optimizer = Adagrad(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    encoder_input_data,\n",
    "    decoder_input_data,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5z6rzmwAmOG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#MAX_LEN = 400\n",
    "\n",
    "def create_masks(input, target):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(input)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(input)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(target)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(target)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_summary(model, tokenizer, input_sequence, max_len, summary_start_token_id, summary_end_token_id):\n",
    "    # Tokenize the input sequence\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_sequence])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Initialize the summary sequence with the start token\n",
    "    summary_sequence = np.zeros((1, max_len))\n",
    "    summary_sequence[0, 0] = summary_start_token_id\n",
    "\n",
    "    # Generate the summary word by word\n",
    "    for i in range(max_len - 1):\n",
    "        # Predict the next word\n",
    "        predictions = model([input_sequence, summary_sequence], training=False)\n",
    "        prediction = predictions[:, i, :]\n",
    "\n",
    "        # Get the predicted word ID\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "\n",
    "        # Stop generating if the predicted word is the end token\n",
    "        if predicted_id == summary_end_token_id:\n",
    "            break\n",
    "\n",
    "        # Add the predicted word to the summary sequence\n",
    "        summary_sequence[:, i+1] = predicted_id\n",
    "\n",
    "    # Convert the summary sequence to text\n",
    "    summary_sequence = summary_sequence[0]\n",
    "    summary = tokenizer.sequences_to_texts([summary_sequence])[0]\n",
    "    summary = summary.replace('<start> ', '')\n",
    "    summary = summary.replace(' <end>', '')\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data\n",
    "test_encoder_input_data = tokenizer.texts_to_sequences(test_df['article'].values.tolist())\n",
    "test_decoder_input_data = tokenizer.texts_to_sequences(test_df['highlights'].values.tolist())\n",
    "test_decoder_target_data = [sequence[1:] for sequence in test_decoder_input_data]\n",
    "test_decoder_target_data.append([0] * MAX_LEN)\n",
    "\n",
    "max_len = 0\n",
    "for sequence in test_encoder_input_data + test_decoder_input_data + test_decoder_target_data:\n",
    "    max_len = max(max_len, len(sequence))\n",
    "\n",
    "test_encoder_input_data = pad_sequences(test_encoder_input_data, maxlen=MAX_LEN, padding='post')\n",
    "test_decoder_input_data = pad_sequences(test_decoder_input_data, maxlen=MAX_LEN, padding='post')\n",
    "test_decoder_target_data = pad_sequences(test_decoder_target_data, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index['<end>'] = len(tokenizer.word_index) +2\n",
    "\n",
    "# Generate the predicted summaries for test data\n",
    "test_predicted_summaries = []\n",
    "summary_start_token_id = tokenizer.word_index.get('<start>')\n",
    "summary_end_token_id = tokenizer.word_index.get('<end>')\n",
    "for i in range(0,10):\n",
    "    article = np.expand_dims(test_encoder_input_data[i], axis=0)\n",
    "    summary = test_decoder_input_data[i]\n",
    "    if len(test_encoder_input_data[i]) > MAX_LEN:\n",
    "        print(f\"Skipping example {i} because article sequence is longer than MAX_LEN\")\n",
    "        continue\n",
    "    try:\n",
    "        predicted_summary = generate_summary(model, tokenizer, article, MAX_LEN, summary_start_token_id, summary_end_token_id)\n",
    "        print(predicted_summary)\n",
    "        if len(predicted_summary) > MAX_LEN:\n",
    "            print(f\"Truncating summary for example {i} because it is longer than MAX_LEN\")\n",
    "            predicted_summary = predicted_summary[:MAX_LEN]\n",
    "        test_predicted_summaries.append(predicted_summary)\n",
    "        #print(predicted_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary for example {i}: {e}\")\n",
    "        print(f\"Article sequence: {article}\")\n",
    "        print(f\"Decoder input sequence: {tokenizer.sequences_to_texts([summary])[0]}\")\n",
    "        print(f\"Encoder input sequence: {tokenizer.sequences_to_texts([test_encoder_input_data[i]])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on ROUGE metric\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(test_predicted_summaries)):\n",
    "    if test_predicted_summaries[i] == test_df['highlights'][i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total\n",
    "\n",
    "# Calculate loss\n",
    "loss = model.evaluate(test_features, test_labels, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"ROUGE scores:\")\n",
    "print(rouge.get_scores(test_predicted_summaries, test_df['highlights'].values.tolist(), avg=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.855 </br>\n",
    "Loss: 0.55</br>\n",
    "ROUGE scores:</br>\n",
    "{'rouge-1': 0.37, 'rouge-2': 0.27, 'rouge-l': 0.33}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
