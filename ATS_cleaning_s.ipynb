{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33aef15",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fd6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bdec24",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30948f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train, test, and validation datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "validation_df = pd.read_csv(\"validation.csv\")\n",
    "\n",
    "\n",
    "# Combine the datasets\n",
    "ats_data = pd.concat([train_df, test_df, validation_df], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "#ats_data.to_csv(\"combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda6bd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311966</th>\n",
       "      <td>e93f721ba4949f21f33549c4a21d55ff456af979</td>\n",
       "      <td>All shops will be allowed to offer ‘click and ...</td>\n",
       "      <td>Shops won't have to apply for planning permiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311967</th>\n",
       "      <td>8df19a570ad14119a7d00f3bbe864fedf8c1691d</td>\n",
       "      <td>Mo Farah has had his nationality called into q...</td>\n",
       "      <td>Mo Farah broke the European half-marathon reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311968</th>\n",
       "      <td>2fdd5f89aa26e91ceea9b0ef264abfcfc3e6fa2e</td>\n",
       "      <td>Wolves kept their promotion hopes alive with a...</td>\n",
       "      <td>Wolves are three points off the play-off place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311969</th>\n",
       "      <td>530d7b18d7a715b368b0745f9dfebfe353adeda8</td>\n",
       "      <td>A Brown University graduate student  has died ...</td>\n",
       "      <td>Hyoun Ju Sohn, a 25-year-old doctoral student,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311970</th>\n",
       "      <td>42d6f6e10a45ff469fbe3ef7345838aa45dcdd61</td>\n",
       "      <td>As thousands of young Australians look to buy ...</td>\n",
       "      <td>60% of estate agents believe owning a home is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311971 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              id  \\\n",
       "0       0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "1       0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2       00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3       0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4       0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "...                                          ...   \n",
       "311966  e93f721ba4949f21f33549c4a21d55ff456af979   \n",
       "311967  8df19a570ad14119a7d00f3bbe864fedf8c1691d   \n",
       "311968  2fdd5f89aa26e91ceea9b0ef264abfcfc3e6fa2e   \n",
       "311969  530d7b18d7a715b368b0745f9dfebfe353adeda8   \n",
       "311970  42d6f6e10a45ff469fbe3ef7345838aa45dcdd61   \n",
       "\n",
       "                                                  article  \\\n",
       "0       By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1       (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2       A drunk driver who killed a young woman in a h...   \n",
       "3       (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4       Fleetwood are the only team still to have a 10...   \n",
       "...                                                   ...   \n",
       "311966  All shops will be allowed to offer ‘click and ...   \n",
       "311967  Mo Farah has had his nationality called into q...   \n",
       "311968  Wolves kept their promotion hopes alive with a...   \n",
       "311969  A Brown University graduate student  has died ...   \n",
       "311970  As thousands of young Australians look to buy ...   \n",
       "\n",
       "                                               highlights  \n",
       "0       Bishop John Folda, of North Dakota, is taking ...  \n",
       "1       Criminal complaint: Cop used his role to help ...  \n",
       "2       Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3       Nina dos Santos says Europe must be ready to a...  \n",
       "4       Fleetwood top of League One after 2-0 win at S...  \n",
       "...                                                   ...  \n",
       "311966  Shops won't have to apply for planning permiss...  \n",
       "311967  Mo Farah broke the European half-marathon reco...  \n",
       "311968  Wolves are three points off the play-off place...  \n",
       "311969  Hyoun Ju Sohn, a 25-year-old doctoral student,...  \n",
       "311970  60% of estate agents believe owning a home is ...  \n",
       "\n",
       "[311971 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d9d2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95369ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.highlights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d694b65",
   "metadata": {},
   "source": [
    "### check if all ID values are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95035924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311971"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa346c1",
   "metadata": {},
   "source": [
    "### dropping the entire ID column since it is insignificant to the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c85d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data = ats_data.drop('id', axis= 1)\n",
    "ats_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0131fd78-9070-40e6-a9af-2b323416c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ats_data.loc[1, 'article'] = np.nan\n",
    "ats_data.loc[122, 'article'] = np.nan\n",
    "ats_data.loc[211, 'article'] = np.nan\n",
    "ats_data.loc[51, 'article'] = np.nan\n",
    "ats_data.loc[278, 'article'] = np.nan\n",
    "ats_data.loc[218, 'article'] = np.nan\n",
    "ats_data.loc[9024, 'article'] = np.nan\n",
    "ats_data.loc[5216, 'article'] = np.nan\n",
    "ats_data.loc[111, 'article'] = np.nan\n",
    "ats_data.loc[222, 'article'] = np.nan\n",
    "ats_data.loc[221, 'article'] = np.nan\n",
    "ats_data.loc[151, 'article'] = np.nan\n",
    "ats_data.loc[2078, 'article'] = np.nan\n",
    "ats_data.loc[2118, 'article'] = np.nan\n",
    "ats_data.loc[90124, 'article'] = np.nan\n",
    "ats_data.loc[51216, 'article'] = np.nan\n",
    "ats_data.loc[6522, 'article'] = np.nan\n",
    "ats_data.loc[21221, 'article'] = np.nan\n",
    "ats_data.loc[52121, 'article'] = np.nan\n",
    "ats_data.loc[27338, 'article'] = np.nan\n",
    "ats_data.loc[21118, 'article'] = np.nan\n",
    "ats_data.loc[90214, 'article'] = np.nan\n",
    "ats_data.loc[52316, 'article'] = np.nan\n",
    "ats_data.loc[2, 'article'] = np.nan\n",
    "ats_data.loc[12, 'article'] = np.nan\n",
    "ats_data.loc[21, 'article'] = np.nan\n",
    "ats_data.loc[513, 'article'] = np.nan\n",
    "ats_data.loc[27, 'article'] = np.nan\n",
    "ats_data.loc[28, 'article'] = np.nan\n",
    "ats_data.loc[924, 'article'] = np.nan\n",
    "ats_data.loc[516, 'article'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65d05c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Incomplete & Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823b633",
   "metadata": {},
   "source": [
    "### Drop rows with missing values in any of the 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60337277-6e45-447f-860d-6aa2021073c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       31\n",
       "highlights     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "ats_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6eb42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 311940 entries, 0 to 311970\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   article     311940 non-null  object\n",
      " 1   highlights  311940 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values in 'id', 'article', or 'highlights' columns\n",
    "ats_data.dropna(subset=['article', 'highlights'], inplace=True)\n",
    "\n",
    "# Display the updated summary of the cleaned dataset\n",
    "print(ats_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2661fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf2f38",
   "metadata": {},
   "source": [
    "## Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8990bc",
   "metadata": {},
   "source": [
    "\n",
    "### Remove any HTML tags from `article` and `highlights` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca65de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the article column contain any HTML links or tags?: False\n",
      "Does the article column contain any HTML links or tags?: False\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a string\n",
    "article_string = str(ats_data['article'])\n",
    "\n",
    "# Check if the article column contains any HTML links or tags\n",
    "article_has_links_or_tags = re.search(r'<a href=\"[^\"]+\">|</a>', article_string) is not None\n",
    "\n",
    "# Print the results\n",
    "print(\"Does the article column contain any HTML links or tags?:\", article_has_links_or_tags)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a string\n",
    "highlights_string = str(ats_data['highlights'])\n",
    "\n",
    "# Check if the article column contains any HTML links or tags\n",
    "highlights_has_links_or_tags = re.search(r'<a href=\"[^\"]+\">|</a>', highlights_string) is not None\n",
    "\n",
    "# Print the results\n",
    "print(\"Does the article column contain any HTML links or tags?:\", highlights_has_links_or_tags)\n",
    "\n",
    "ats_data['article'] = ats_data['article'].apply(lambda x: re.sub(r'<.*?|>', '', x))\n",
    "ats_data['highlights'] = ats_data['highlights'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "\n",
    "# Display the updated summary of the cleaned dataset\n",
    "print(ats_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6500e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 311940 entries, 0 to 311970\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   article     311940 non-null  object\n",
      " 1   highlights  311940 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ats_data['article'] = ats_data['article'].apply(lambda x: re.sub(r'<.*?|>', '', x))\n",
    "ats_data['highlights'] = ats_data['highlights'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "\n",
    "# Display the updated summary of the cleaned dataset\n",
    "print(ats_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c64daa",
   "metadata": {},
   "source": [
    "### Removing '\\n' newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139bfa6",
   "metadata": {},
   "source": [
    "#### before removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8783810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.highlights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40076705",
   "metadata": {},
   "outputs": [],
   "source": [
    "ats_data['article'] = ats_data['article'].apply(lambda x: re.sub(r' .\\n', '. ', x))\n",
    "ats_data['highlights'] = ats_data['highlights'].apply(lambda x: re.sub(r' .\\n', '. ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f73ed",
   "metadata": {},
   "source": [
    "#### after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9256af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed. He contracted the infection through contaminated food in Italy. Church members in Fargo, Grand Forks and Jamestown could have been exposed .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.highlights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f549f",
   "metadata": {},
   "source": [
    "### Removing all special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdefb4",
   "metadata": {},
   "source": [
    "#### Before removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4221f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By . Daily Mail Reporter . This is the moment a train announcer stunned passengers by announcing over a tannoy as they pulled into a station to beware of pickpockets and gipsies. The London Midland service had been pulling into Telford Station, Shropshire, on Saturday when the comments were made. Passenger Chris Downes, 46, was recording on his mobile at the time and the announcer can clearly be heard saying: \\'Telford Central - please be aware of pickpockets and gipsies\\'. Scroll down for video . This is the moment a train announcer stunned passengers by announcing over a tannoy as they pulled into a station to beware of pickpockets and gipsies . The remark was mainly greeted by cheers from Shrewsbury Town football fans travelling back from their game against Wolverhampton Wanderers. But London Midland said it is now launching an investigation into the incident on board the 17.25 Wolverhampton to Shrewsbury service. Yesterday Wolves fan Mr Downes, who was on his way home to Bayston Hill, Shropshire, with son Jack, 14, said: \\'There had been loads of banter between the fans sharing carriages, which threatened to boil over. The atmosphere was a bit hostile at times. \\'The announcement diluted the situation quite a bit and helped lighten the mood, to be honest.\\'But I thought at the time he might get into a bit of trouble for it. Which is shame really, because I’m sure it was intended in good humour. \\'When we got to Shrewsbury he said \"Welcome back to civilisation\" and I for one am looking forward to travelling on his train again in future. \\'There’s not enough train drivers with a sense of humour and I think his comments were only made in jest.\\' However, other passengers and residents of Telford yesterday reacted with disgust at the \\'unprofessional\\' and \\'offensive\\' comments. Mark Peaker, 47, a father-of-three, from Telford said: \\'I couldn’t believe what I was hearing - they have not only used a derogatory term they have managed to offend an entire town. \\'It suggests we are just a town full of thieves, which is not the case at all. Somebody in a professional role should not be insulting places while they are working. London Midland said it is now launching an investigation into the incident on board the 17.25 Wolverhampton to Shrewsbury service . \\'I’m all for them having a sense of humour but this was not funny at all and I hope he is disciplined for his unprofessional actions.\\' One Wolves fan, who lives in Telford but wished to remain anonymous, was travelling back home from the derby match at Molineux, which ended 0-0. He said: \\'I couldn’t believe it. I was utterly flabbergasted. \\'Tensions among fans were already high after the match and I don’t think that helped the situation at all. The London Midland service had been pulling into Telford Station, Shropshire, on Saturday when the comments were made . \\'Telford is actually a really nice place to live. It certainly isn’t up to a train announcer to make insulting comments about it.\\' The Gipsy Council called for the matter to be taken up with the police and branded the remarks as racist. Bill Kerswell, a spokesman for the council, said: \\'This is unlawful, it is a racist comment. \\'It is the same as using any offensive word relating to homosexuals or people of colour. \\'I would think it is a police matter and I hope they take it up and look into it.\\' A spokesman for the train company thanked passengers for drawing it to their attention and added: \\'We do not tolerate any sort of comment of that kind made by anyone on our trains and will be looking into it immediately.\\''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.article[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bd03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^\\w\\s,.!?()\\’]')\n",
    "ats_data['article'] = ats_data['article'].str.replace(regex, '')\n",
    "ats_data['highlights'] = ats_data['highlights'].str.replace(regex, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "357d958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By . Daily Mail Reporter . This is the moment a train announcer stunned passengers by announcing over a tannoy as they pulled into a station to beware of pickpockets and gipsies. The London Midland service had been pulling into Telford Station, Shropshire, on Saturday when the comments were made. Passenger Chris Downes, 46, was recording on his mobile at the time and the announcer can clearly be heard saying Telford Central  please be aware of pickpockets and gipsies. Scroll down for video . This is the moment a train announcer stunned passengers by announcing over a tannoy as they pulled into a station to beware of pickpockets and gipsies . The remark was mainly greeted by cheers from Shrewsbury Town football fans travelling back from their game against Wolverhampton Wanderers. But London Midland said it is now launching an investigation into the incident on board the 17.25 Wolverhampton to Shrewsbury service. Yesterday Wolves fan Mr Downes, who was on his way home to Bayston Hill, Shropshire, with son Jack, 14, said There had been loads of banter between the fans sharing carriages, which threatened to boil over. The atmosphere was a bit hostile at times. The announcement diluted the situation quite a bit and helped lighten the mood, to be honest.But I thought at the time he might get into a bit of trouble for it. Which is shame really, because I’m sure it was intended in good humour. When we got to Shrewsbury he said Welcome back to civilisation and I for one am looking forward to travelling on his train again in future. There’s not enough train drivers with a sense of humour and I think his comments were only made in jest. However, other passengers and residents of Telford yesterday reacted with disgust at the unprofessional and offensive comments. Mark Peaker, 47, a fatherofthree, from Telford said I couldn’t believe what I was hearing  they have not only used a derogatory term they have managed to offend an entire town. It suggests we are just a town full of thieves, which is not the case at all. Somebody in a professional role should not be insulting places while they are working. London Midland said it is now launching an investigation into the incident on board the 17.25 Wolverhampton to Shrewsbury service . I’m all for them having a sense of humour but this was not funny at all and I hope he is disciplined for his unprofessional actions. One Wolves fan, who lives in Telford but wished to remain anonymous, was travelling back home from the derby match at Molineux, which ended 00. He said I couldn’t believe it. I was utterly flabbergasted. Tensions among fans were already high after the match and I don’t think that helped the situation at all. The London Midland service had been pulling into Telford Station, Shropshire, on Saturday when the comments were made . Telford is actually a really nice place to live. It certainly isn’t up to a train announcer to make insulting comments about it. The Gipsy Council called for the matter to be taken up with the police and branded the remarks as racist. Bill Kerswell, a spokesman for the council, said This is unlawful, it is a racist comment. It is the same as using any offensive word relating to homosexuals or people of colour. I would think it is a police matter and I hope they take it up and look into it. A spokesman for the train company thanked passengers for drawing it to their attention and added We do not tolerate any sort of comment of that kind made by anyone on our trains and will be looking into it immediately.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ats_data.article[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8c8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'article' \n",
    "\n",
    "# Check for duplicates in the specified column\n",
    "duplicate_values = ats_data.duplicated(subset=column_name, keep=False)\n",
    "\n",
    "# Filter the DataFrame to show only the duplicate rows\n",
    "duplicate_rows = ats_data[duplicate_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "011c12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=pd.DataFrame(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "516583f8-7a23-4213-88ce-3660bb3063c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6214, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6566179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ats_data.drop_duplicates(subset=column_name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d4906b2-a3ab-4732-bd59-dbae87c37c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'article' \n",
    "\n",
    "# Check for duplicates in the specified column\n",
    "duplicate_values_ = ats_data.duplicated(subset=column_name, keep=False)\n",
    "\n",
    "# Filter the DataFrame to show only the duplicate rows\n",
    "duplicate_rows_ = ats_data[duplicate_values_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e3f4745-d078-4f79-8b8b-436cc2da4c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_recheck=pd.DataFrame(duplicate_rows_)\n",
    "duplicates_recheck.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904f5df",
   "metadata": {},
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86367e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convert all text to lowercase in `article` and `highlights` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ffece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase in 'article' and 'highlights' column\n",
    "ats_data['article'] = ats_data['article'].str.lower()\n",
    "ats_data['highlights'] = ats_data['highlights'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d4b74",
   "metadata": {},
   "source": [
    "### Expanding all the contractions in the `article` and `highlights` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5912801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map contractions to their expanded forms\n",
    "contraction_map = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return ' '.join([contraction_map.get(word, word) for word in text.split()])\n",
    "\n",
    "# Apply the function to the 'article' and 'highlights' columns in the dataframe\n",
    "ats_data['article'] = ats_data['article'].apply(expand_contractions)\n",
    "ats_data['highlights'] = ats_data['highlights'].apply(expand_contractions)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "#ats_data.to_csv('cleaned.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "637eff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "ats_data.to_csv('cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7b487d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Calculate article lengths\\nats_data['article_length'] = ats_data['article'].str.split().apply(len)\\n\\n# Bar chart of article lengths with custom colors\\nplt.figure(figsize=(10, 6))\\nsns.histplot(ats_data['article_length'], bins=20, color='#1f77b4')  # Set custom color\\nplt.axvline(ats_data['article_length'].mean(), color='red', linestyle='--', label='Mean')  # Add a vertical line for mean article length\\nplt.xlabel('Article Length')\\nplt.ylabel('Count')\\nplt.title('Distribution of Article Lengths')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "'''# Calculate article lengths\n",
    "ats_data['article_length'] = ats_data['article'].str.split().apply(len)\n",
    "\n",
    "# Bar chart of article lengths with custom colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ats_data['article_length'], bins=20, color='#1f77b4')  # Set custom color\n",
    "plt.axvline(ats_data['article_length'].mean(), color='red', linestyle='--', label='Mean')  # Add a vertical line for mean article length\n",
    "plt.xlabel('Article Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Article Lengths')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c09153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from wordcloud import WordCloud\\n\\nplt.figure(figsize=(10, 8))\\nwordcloud = WordCloud(background_color='whitesmoke', width=800, height=400, max_words=100).generate(' '.join(ats_data['highlights']))\\nplt.imshow(wordcloud, interpolation='bilinear')\\nplt.axis('off')\\nplt.title('Word Cloud of Highlights')\\nplt.show()\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "\n",
    "# Example: Word cloud of highlights\n",
    "'''from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "wordcloud = WordCloud(background_color='whitesmoke', width=800, height=400, max_words=100).generate(' '.join(ats_data['highlights']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Highlights')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a5aafb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.figure(figsize=(10, 8))\\nwordcloud = WordCloud(background_color='whitesmoke', width=800, height=400, max_words=100).generate(' '.join(ats_data['article']))\\nplt.imshow(wordcloud, interpolation='bilinear')\\nplt.axis('off')\\nplt.title('Word Cloud of article')\\nplt.show()\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "\n",
    "'''plt.figure(figsize=(10, 8))\n",
    "wordcloud = WordCloud(background_color='whitesmoke', width=800, height=400, max_words=100).generate(' '.join(ats_data['article']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of article')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "382e0fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ats_data['highlights_length'] = ats_data['highlights'].str.split().apply(len)\\n\\nplt.figure(figsize=(8, 6))\\nsns.scatterplot(x='article_length', y='highlights_length', data=ats_data)\\nplt.xlabel('Article Length')\\nplt.ylabel('Highlight Length')\\nplt.title('Scatter Plot of Article Length vs Highlights Length')\\nplt.show()\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "\n",
    "# Example: Scatter plot of article length vs highlight length\n",
    "'''ats_data['highlights_length'] = ats_data['highlights'].str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='article_length', y='highlights_length', data=ats_data)\n",
    "plt.xlabel('Article Length')\n",
    "plt.ylabel('Highlight Length')\n",
    "plt.title('Scatter Plot of Article Length vs Highlights Length')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b703e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"correlation = ats_data[['article_length', 'highlights_length']].corr()\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(correlation, annot=True, cmap='coolwarm')\\nplt.title('Correlation between Article Length and Highlight Length')\\nplt.show()\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "\n",
    "'''correlation = ats_data[['article_length', 'highlights_length']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation between Article Length and Highlight Length')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "060dad99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.figure(figsize=(10, 6))\\nplt.plot(ats_data['highlights_length'], label='Highlights Length')\\nplt.plot(ats_data['article_length'], label='Article Length')\\nplt.xlabel('Index')\\nplt.ylabel('Length')\\nplt.title('Length of Highlights and Articles')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment later\n",
    "\n",
    "# Create a line graph with two lines\n",
    "'''plt.figure(figsize=(10, 6))\n",
    "plt.plot(ats_data['highlights_length'], label='Highlights Length')\n",
    "plt.plot(ats_data['article_length'], label='Article Length')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Length of Highlights and Articles')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1beb8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6541403e-d772-4aeb-a827-3d9b8648f836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n\\n# Define a function to calculate the percentage of nouns and pronouns in a given text\\ndef calc_noun_pronoun_percentage(text):\\n    # Tokenize the text into words\\n    words = word_tokenize(text)\\n    # Remove stop words from the list of words\\n    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\\n    # Get the part-of-speech (POS) tags for the filtered words\\n    pos_tags = nltk.pos_tag(filtered_words)\\n    # Count the number of nouns and pronouns in the POS tags\\n    noun_count = sum([1 for tag in pos_tags if tag[1] == 'NN' or tag[1] == 'NNS' or tag[1] == 'NNP' or tag[1] == 'NNPS'])\\n    pronoun_count = sum([1 for tag in pos_tags if tag[1] == 'PRP' or tag[1] == 'PRP$'])\\n    # Calculate the percentage of nouns and pronouns in the text\\n    total_count = noun_count + pronoun_count\\n    noun_percentage = round((noun_count / total_count) * 100, 2) if total_count > 0 else 0.0\\n    pronoun_percentage = round((pronoun_count / total_count) * 100, 2) if total_count > 0 else 0.0\\n    # Return the percentage of nouns and pronouns\\n    return (noun_percentage, pronoun_percentage)\\n\\n# Calculate the percentage of nouns and pronouns in the highlights column\\nats_data['highlight_noun_percentage'], ats_data['highlight_pronoun_percentage'] = zip(*ats_data['highlights'].apply(calc_noun_pronoun_percentage))\\n\\n# Plot a bar chart of the noun and pronoun percentages\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x=['Nouns', 'Pronouns'], y=[ats_data['highlight_noun_percentage'].mean(), ats_data['highlight_pronoun_percentage'].mean()], palette='coolwarm')\\nplt.ylim(0, 100)\\nplt.xlabel('Part of Speech')\\nplt.ylabel('Percentage')\\nplt.title('Percentage of Nouns and Pronouns in the Highlights')\\nplt.show()\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a function to calculate the percentage of nouns and pronouns in a given text\n",
    "def calc_noun_pronoun_percentage(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    # Get the part-of-speech (POS) tags for the filtered words\n",
    "    pos_tags = nltk.pos_tag(filtered_words)\n",
    "    # Count the number of nouns and pronouns in the POS tags\n",
    "    noun_count = sum([1 for tag in pos_tags if tag[1] == 'NN' or tag[1] == 'NNS' or tag[1] == 'NNP' or tag[1] == 'NNPS'])\n",
    "    pronoun_count = sum([1 for tag in pos_tags if tag[1] == 'PRP' or tag[1] == 'PRP$'])\n",
    "    # Calculate the percentage of nouns and pronouns in the text\n",
    "    total_count = noun_count + pronoun_count\n",
    "    noun_percentage = round((noun_count / total_count) * 100, 2) if total_count > 0 else 0.0\n",
    "    pronoun_percentage = round((pronoun_count / total_count) * 100, 2) if total_count > 0 else 0.0\n",
    "    # Return the percentage of nouns and pronouns\n",
    "    return (noun_percentage, pronoun_percentage)\n",
    "\n",
    "# Calculate the percentage of nouns and pronouns in the highlights column\n",
    "ats_data['highlight_noun_percentage'], ats_data['highlight_pronoun_percentage'] = zip(*ats_data['highlights'].apply(calc_noun_pronoun_percentage))\n",
    "\n",
    "# Plot a bar chart of the noun and pronoun percentages\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=['Nouns', 'Pronouns'], y=[ats_data['highlight_noun_percentage'].mean(), ats_data['highlight_pronoun_percentage'].mean()], palette='coolwarm')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel('Part of Speech')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Nouns and Pronouns in the Highlights')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f9fe2f3-1fa1-48d8-965b-69ed90d50385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Calculate the percentage of nouns and pronouns in the article column\\nats_data['article_noun_percentage'], ats_data['article_pronoun_percentage'] = zip(*ats_data['article'].apply(calc_noun_pronoun_percentage))\\n\\n# Plot a bar chart of the noun and pronoun percentages\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x=['Nouns', 'Pronouns'], y=[ats_data['article_noun_percentage'].mean(), ats_data['article_pronoun_percentage'].mean()], palette='coolwarm')\\nplt.ylim(0, 100)\\nplt.xlabel('Part of Speech')\\nplt.ylabel('Percentage')\\nplt.title('Percentage of Nouns and Pronouns in the Article')\\nplt.show()\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Calculate the percentage of nouns and pronouns in the article column\n",
    "ats_data['article_noun_percentage'], ats_data['article_pronoun_percentage'] = zip(*ats_data['article'].apply(calc_noun_pronoun_percentage))\n",
    "\n",
    "# Plot a bar chart of the noun and pronoun percentages\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=['Nouns', 'Pronouns'], y=[ats_data['article_noun_percentage'].mean(), ats_data['article_pronoun_percentage'].mean()], palette='coolwarm')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel('Part of Speech')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Nouns and Pronouns in the Article')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2f1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b851d013",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30ef48",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe9cce",
   "metadata": {},
   "source": [
    "#### Data Augmentation using Rule based techniques on cleaned train split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12cc10",
   "metadata": {},
   "source": [
    "#### Main idea is to take 20% of train data to apply augmentation using synonym replacement technique and add augmented data to the original train split so that both original data of that 20% and augmentated data will be present which increases the vocabulary of the data. Same applies to Random deletion and random swap techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "283c02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nlpaug\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7f40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5cf61d3",
   "metadata": {},
   "source": [
    "### Splitting the cleaned dataset into train, test and validation sets before applying augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b6198c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(ats_data) * 0.9)\n",
    "test_size = int(len(ats_data) * 0.05)\n",
    "val_size = len(ats_data) - train_size - test_size\n",
    "\n",
    "train_df = ats_data.iloc[:train_size]\n",
    "test_df = ats_data.iloc[train_size:train_size + test_size]\n",
    "val_df = ats_data.iloc[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29fe11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (277945, 2)\n",
      "test_df (15441, 2)\n",
      "val_df (15442, 2)\n"
     ]
    }
   ],
   "source": [
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "print('val_df', val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43d3d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by . associated press . published . 1411 est, ...</td>\n",
       "      <td>bishop john folda, of north dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(cnn) with a breezy sweep of his pen president...</td>\n",
       "      <td>nina dos santos says europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fleetwood are the only team still to have a 10...</td>\n",
       "      <td>fleetwood top of league one after 20 win at sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hes been accused of making many a fashion faux...</td>\n",
       "      <td>prime minister and his family are enjoying an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  by . associated press . published . 1411 est, ...   \n",
       "3  (cnn) with a breezy sweep of his pen president...   \n",
       "4  fleetwood are the only team still to have a 10...   \n",
       "5  hes been accused of making many a fashion faux...   \n",
       "\n",
       "                                          highlights  \n",
       "0  bishop john folda, of north dakota, is taking ...  \n",
       "3  nina dos santos says europe must be ready to a...  \n",
       "4  fleetwood top of league one after 20 win at sc...  \n",
       "5  prime minister and his family are enjoying an ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "941448ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280947</th>\n",
       "      <td>(cnn) the remake of the movie sparkle seemed t...</td>\n",
       "      <td>whitney houston took her first movie role in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280948</th>\n",
       "      <td>over 1,000 people have been left homeless afte...</td>\n",
       "      <td>firefighters battled a huge fire wednesday nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280949</th>\n",
       "      <td>(cnn) duwon steven clark is standing on the wh...</td>\n",
       "      <td>haitian american duwon steven clark returned t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280950</th>\n",
       "      <td>(cnn) seeing more mustaches this month? many o...</td>\n",
       "      <td>movember encourages men to grow mustaches to r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  article  \\\n",
       "280947  (cnn) the remake of the movie sparkle seemed t...   \n",
       "280948  over 1,000 people have been left homeless afte...   \n",
       "280949  (cnn) duwon steven clark is standing on the wh...   \n",
       "280950  (cnn) seeing more mustaches this month? many o...   \n",
       "\n",
       "                                               highlights  \n",
       "280947  whitney houston took her first movie role in 1...  \n",
       "280948  firefighters battled a huge fire wednesday nig...  \n",
       "280949  haitian american duwon steven clark returned t...  \n",
       "280950  movember encourages men to grow mustaches to r...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5800e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296526</th>\n",
       "      <td>rating . poets house is a swish new hotel in e...</td>\n",
       "      <td>poets house is a swish new hotel in ely, three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296527</th>\n",
       "      <td>dani alves looks set to leave barcelona this s...</td>\n",
       "      <td>dani alves has spent seven seasons with the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296528</th>\n",
       "      <td>a libyan rapper has released a new music video...</td>\n",
       "      <td>calling himself volcano rapper is a 31yearold ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296529</th>\n",
       "      <td>theres no denying that the british interior ae...</td>\n",
       "      <td>the landgate cottage in rye is outfitted in an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  article  \\\n",
       "296526  rating . poets house is a swish new hotel in e...   \n",
       "296527  dani alves looks set to leave barcelona this s...   \n",
       "296528  a libyan rapper has released a new music video...   \n",
       "296529  theres no denying that the british interior ae...   \n",
       "\n",
       "                                               highlights  \n",
       "296526  poets house is a swish new hotel in ely, three...  \n",
       "296527  dani alves has spent seven seasons with the ca...  \n",
       "296528  calling himself volcano rapper is a 31yearold ...  \n",
       "296529  the landgate cottage in rye is outfitted in an...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fb809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9edc4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6376775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by . associated press . published . 1411 est, ...</td>\n",
       "      <td>bishop john folda, of north dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(cnn) with a breezy sweep of his pen president...</td>\n",
       "      <td>nina dos santos says europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fleetwood are the only team still to have a 10...</td>\n",
       "      <td>fleetwood top of league one after 20 win at sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hes been accused of making many a fashion faux...</td>\n",
       "      <td>prime minister and his family are enjoying an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  by . associated press . published . 1411 est, ...   \n",
       "1  (cnn) with a breezy sweep of his pen president...   \n",
       "2  fleetwood are the only team still to have a 10...   \n",
       "3  hes been accused of making many a fashion faux...   \n",
       "\n",
       "                                          highlights  \n",
       "0  bishop john folda, of north dakota, is taking ...  \n",
       "1  nina dos santos says europe must be ready to a...  \n",
       "2  fleetwood top of league one after 20 win at sc...  \n",
       "3  prime minister and his family are enjoying an ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f383c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(cnn) the remake of the movie sparkle seemed t...</td>\n",
       "      <td>whitney houston took her first movie role in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over 1,000 people have been left homeless afte...</td>\n",
       "      <td>firefighters battled a huge fire wednesday nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(cnn) duwon steven clark is standing on the wh...</td>\n",
       "      <td>haitian american duwon steven clark returned t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(cnn) seeing more mustaches this month? many o...</td>\n",
       "      <td>movember encourages men to grow mustaches to r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  (cnn) the remake of the movie sparkle seemed t...   \n",
       "1  over 1,000 people have been left homeless afte...   \n",
       "2  (cnn) duwon steven clark is standing on the wh...   \n",
       "3  (cnn) seeing more mustaches this month? many o...   \n",
       "\n",
       "                                          highlights  \n",
       "0  whitney houston took her first movie role in 1...  \n",
       "1  firefighters battled a huge fire wednesday nig...  \n",
       "2  haitian american duwon steven clark returned t...  \n",
       "3  movember encourages men to grow mustaches to r...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a265a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating . poets house is a swish new hotel in e...</td>\n",
       "      <td>poets house is a swish new hotel in ely, three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dani alves looks set to leave barcelona this s...</td>\n",
       "      <td>dani alves has spent seven seasons with the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a libyan rapper has released a new music video...</td>\n",
       "      <td>calling himself volcano rapper is a 31yearold ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theres no denying that the british interior ae...</td>\n",
       "      <td>the landgate cottage in rye is outfitted in an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  rating . poets house is a swish new hotel in e...   \n",
       "1  dani alves looks set to leave barcelona this s...   \n",
       "2  a libyan rapper has released a new music video...   \n",
       "3  theres no denying that the british interior ae...   \n",
       "\n",
       "                                          highlights  \n",
       "0  poets house is a swish new hotel in ely, three...  \n",
       "1  dani alves has spent seven seasons with the ca...  \n",
       "2  calling himself volcano rapper is a 31yearold ...  \n",
       "3  the landgate cottage in rye is outfitted in an...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eecdaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('cleaned_test.csv', index=False)\n",
    "val_df.to_csv('cleaned_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c680dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21de4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64f96005",
   "metadata": {},
   "source": [
    "### 1. Synonym Replacement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352abf6d",
   "metadata": {},
   "source": [
    "#### taking a random 20% sample from train split for augmentation using 1st technique (Synonym Replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58139571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of data to augment\n",
    "augment_percentage = 0.2\n",
    "\n",
    "#dataframe to store 20% data taken from train split\n",
    "twenty_percent_df = pd.DataFrame()\n",
    "twenty_percent_df = train_df.sample(frac=augment_percentage)\n",
    "\n",
    "#new dataframe to store augmented data\n",
    "augmented_train_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15ca7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55589, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34623f",
   "metadata": {},
   "source": [
    "#### Before Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f404017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55589 entries, 119807 to 267532\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   article     55589 non-null  object\n",
      " 1   highlights  55589 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "twenty_percent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f994c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_percent_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "819bf872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by . steve nolan . published . 0653 est, 18 july 2013 . . updated . 0654 est, 18 july 2013 . a new zealand woman allegedly bombarded the bestselling author of a book on the harry potter series with threats and abuse, according to a federal court complaint. jessica elizabeth parker is said to have sent a barrage of abuse over a five year period to new yorkbased harry, a history author melissa anelli. ms parker is accused of threatening to slit the throat of ms anellli and is even said to have got a tattoo of the potter experts face, according to the new york post. victim harry potter expert and bestselling author melissa anelli who has allegedly been stalked by a new zealand woman for the past five years . it is claimed that the abuse began in 2008 when ms parker was banned from ms anellis the leaky cauldron fan website after writing abusive and violent posts about harry potter actress emma watson. according to the new york daily news, the complaint file states that ms parker wrote on a postcode sent to ms anellis parents as long as you refuse to deal with me, ill remain that little demon on your shoulder. i cant be denied forever...else i will return to your home and flay you alive. the fbi has secured a warrant for ms parkers arrest. miss anelli welcomed the news, saying in a statement on the leaky cauldron site earlier today it has been a long five years of continuous threats and abuse, not only to me but my family, friends, and colleagues. i only hope it’s carried through to justice being served. bestseller melissa anelli wrote harry, a history, left, a book written about j.k rowlings series of books and subsequent film adaptations starring daniel radcliffe, right . i want to thank the leaky community for its constant support over the past decadeplus, for its support now, and for its understanding while i refrain from discussing particulars in order to assure this sees a proper end. she had told the new york post yesterday it’s awful what she’s done to me. there is no structure in place to help you when someone overseas stalks you. ms anelli has been running the leaky cauldron site since 2001. the former news reporters book has been published in eight different countries and in four languages. she is currently working on her second nonfiction book. sorry we are unable to accept comments for legal reasons.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df.article[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83376480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fbi has obtained a warrant for the arrest of jessica elizabeth parker. she allegedly abused harry, a history author melissa anelli over five years. ms parker is said to have threatened brooklynbased ms anellis family. harry potter expert ms anelli welcomed the development today .'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df.highlights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344be823",
   "metadata": {},
   "source": [
    "#### Function definition of Synonym replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd5bb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nlpaug\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Define the percentage of data to augment\n",
    "augment_percentage = 0.2\n",
    "\n",
    "#dataframe to store 20% data taken from train split\n",
    "twenty_percent_df = pd.DataFrame()\n",
    "twenty_percent_df = train_df.sample(frac=augment_percentage)\n",
    "\n",
    "#new dataframe to store augmented data\n",
    "augmented_train_df = pd.DataFrame()\n",
    "\n",
    "twenty_percent_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Define the augmentation functions\n",
    "def augment_synonym(text):\n",
    "    # Create an augmentation object\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    # Augment the text\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "# Apply the augmentation functions to the test data\n",
    "\n",
    "augmented_train_df['article'] = twenty_percent_df['article'].apply(augment_synonym)\n",
    "augmented_train_df['highlights'] = twenty_percent_df['highlights'].apply(augment_synonym)\n",
    "\n",
    "#appending the augmented rows to main cleaned train split df(test_df)from augmented_test_df\n",
    "train_df = train_df.append(augmented_train_df, ignore_index=True)\n",
    "\n",
    "# Save the augmented+train data to a CSV file for backup\n",
    "train_df.to_csv('augmented_train1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9842ff",
   "metadata": {},
   "source": [
    "#### calling the augmentation function and storing the results in augmented_test_df and then appending to the main train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36c84437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/xqdd6m4n2p56hnvg1t8_81_c0000gn/T/ipykernel_8955/1275962899.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(augmented_train_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Apply the augmentation functions to the test data\n",
    "\n",
    "augmented_train_df['article'] = twenty_percent_df['article'].apply(augment_synonym)\n",
    "augmented_train_df['highlights'] = twenty_percent_df['highlights'].apply(augment_synonym)\n",
    "\n",
    "#appending the augmented rows to main cleaned train split df(test_df)from augmented_test_df\n",
    "train_df = train_df.append(augmented_train_df, ignore_index=True)\n",
    "\n",
    "# Save the augmented+train data to a CSV file for backup\n",
    "train_df.to_csv('augmented_train1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c6a6d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'highlights'], dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "211fbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76712156",
   "metadata": {},
   "source": [
    "#### After Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4904c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by. steve nolan. published. 0653 est, 18 july 2013. . updated. 0654 est, 18 july 2013. a new zealand woman allegedly bombarded the bestselling author of a book on the harry potter series with threats and abuse, according to a federal court complaint. jessica elizabeth parker is said to have sent a barrage of abuse over a five year period to new yorkbased harry, a history author melissa anelli. ms parker is accused of threatening to slit the throat of ms. anellli and is even said to have got a tattoo of the potter experts face, according to the new york post. victim harry potter expert and bestselling author melissa anelli who has allegedly been stalked by a new zealand woman for the past five years. it is claimed that the abuse began in 2008 when ms parker was banned from ms anellis the leaky cauldron fan website after writing abusive and violent posts about harry potter actress emma watson. according to the new york daily news, the complaint file state that ms parker wrote on a postcode sent to ms anellis parents as long as you refuse to deal with me, ill remain that little demon on your shoulder. i cant be denied forever. .. else i will return to your home and flay you alive. the fbi has secured a warrant for ms parkers arrest. miss anelli welcomed the news, saying in a statement on the leaky cauldron site earlier today it has been a long five years of continuous threats and abuse, not only to me but my family, friends, and colleagues. i exclusively hope it ’ s carried through to justice being served. bestseller melissa anelli wrote harry, a history, left, a bible written about j. k rowlings series of books and subsequent film adaptations starring daniel radcliffe, right. i want to thank the leaky community for its constant support over the past decadeplus, for its support now, and for its agreement while i refrain from discussing particulars in order to assure this sees a proper end. she had told the new york post yesterday it ’ s awful what she ’ s done to me. there is no structure in place to help you when someone overseas stalks you. ms anelli has been running the leaky cauldron website since 2001. the former news reporters book has been published in eight different countries and in four languages. she is currently working on her second nonfiction book. sorry we are unable to accept comments for legal reasons.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df.article[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd813d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the federal bureau of investigation has prevail a warrantee for the arrest of jessica elizabeth ii charlie parker. she allegedly abused harry, a account author melissa anelli over five class. ms parker is sound out to have peril brooklynbased ms anellis family. harry potter expert ms anelli welcomed the development today.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df.highlights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acad4a",
   "metadata": {},
   "source": [
    "### 2. Random Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43035b",
   "metadata": {},
   "source": [
    "#### taking a random 20% sample from train split for augmentation using 2nd technique (Random Deletion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a676171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of data to augment\n",
    "augment_percentage = 0.2\n",
    "\n",
    "#new dataframe to store augmented data\n",
    "augmented_train_df2 = pd.DataFrame(columns=['article','highlights'])\n",
    "\n",
    "#dataframe to store 20% data taken from train split\n",
    "twenty_percent_df2 = pd.DataFrame()\n",
    "twenty_percent_df2 = train_df.sample(frac=augment_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bfed5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_percent_df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5189a8",
   "metadata": {},
   "source": [
    "#### Before Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d3d2604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( cnn) sometimes the price of success is losing the thing you most cherish. for charlotte dujardin, every triumph in the sporting arena seemingly took her another footprint closer to an uncertain future. billed as the girl with the dancing horse after her heroics at the london 2012 olympics, she feared that every time she competed would be the last waltz with her equine partner. the mere thought of losing her best friend reduced dujardin to tears. valegro, the horse on which she won double gold, on which she was crowned double european champion and on which she aims to repeat the feat at augusts world championships, was attracting big offers. he was valued at 10 million but, after much speculation, valegros for sale sign has finally come down meaning they now have the chance to continue a remarkable recordsetting run together. his future is secure he is never going to be sold, we have him forever, the british rider says of the 12yearold horse, which is coowned by her mentor and fellow olympic champion carl hester and roly luard. its an unbelievable relief. it was really, really hard knowing that every competition we did might be our last one. i never knew what was going on, i was too afraid to ask, dujardin tells cnn. hester, who won team dressage gold alongside dujardin at london 2012, hopes to set up a small syndicate to cover the costs of keeping valegro in competition. there will be others i can sell on but this matchless is special, the 46yearold told horse and country earlier this year. i didnt want charlotte to feel like he might be sold its not great to ride under those conditions. besides, valegro has been with us since he was two so whoever we sell him to, are they going to look after him the way we do? probably not. dujardin describes valegro more affectionately known as blueberry at his yard as a ferrari and, to all intents and purposes, she is a fernando alonso as she wrestles the ultimate horsepower out of him. hes a real hot horse who likes to work, and hes just the most willing and comfortable horse to ride, she says of their telepathic partnership. he trusts me implicitly, we both have that trust for each other. as for me, i just do what i do. i just have a feeling inside me and i cant wholly explain that feeling. dujardin is the world no. 1 in dressage, which is split into two upshot (the set routine and the freestyle) and holds the world records in all three elements of her play the freestyle, grand prix and grand prix special. her most recent record came at last months reem acra fei world cup final in lyon with a score of 87. 129 comfortably beating her previous milestone of 85. 942 set at the european championships. she scored 92. 179 in the freestyle section. surely there is a ceiling to the records? well, youd think that but he just gets better every time, dujardin says. i dont know what the limit is although i do believe we can at least beat the freestyle record. there still feels like theres more to come. dujardin is somewhat disbelieving at the success that she has achieved. it is true that she hasnt gone down the typical route. she did not have the privileged upbringing of many of her rivals. born in enfield, north london, she only got her chance when her grandmother joy died, leaving a fivefigure inheritance to give dujardin her the chance to buy her first horse of note. its odd when i look back, i have to pinch myself sometimes, i honestly cant believe it, she says. ive done more than i ever dreamed of. ive just worked hard to get to the top. ive had so many letters from people saying ive inspired them to take up riding and thats an amazing feeling. getting to the top is one thing but staying there is quite another. thats the difficult bit. dujardin will take a trip down memory lane on friday when competing at the royal windsor horse show, one of the first events that she ever competed in as a child and a chance to display her and valegros finery on home soil once more. her biggest home display will forever be the london 2012 olympics. the british team had entered it looking for a first olympic medal in the discipline in over a century, and she ended up picking up two golds) in the space of just a week. but she admits life after london was tough. the whole experience was incredible. id just aimed to be there so that was it, id achieved my goal. to get those gold medals was just an unbelievable bonus. but after that died down i almost felt a bit depressed understandable after such a high. it was difficult coming from doing and then little media to suddenly personify in the media spotlight. people wanted me to do everything, and i found that really, really tough. it was a very difficult time. part of that was down to the uncertainty of not knowing valegros future but obviously thats all changed. throughout it all, the pair kept on winning and breaking records, but the complete happiness has fully returned in 2014. in addition, the olympics has brought her all manner of other unlikely facets to her life, ranging from a friendship with charttopping singer leona lewis, who has come to see her train and compete, to passing on tips to former england footballer michael owen on dressage for his daughter, who also rides. none of that has fazed dujardin, who still bears the same trademark smile that delighted the crowds in london and has made her something of a smiling assassin in which she has decimated her rivals in competition. just the world title now is left on the todo list. should she win that, she says i will have achieved everything in my career at the age of 28. itll be time to retire! but there are no retirement plans. she and valegro have plenty of competitions ahead of them. just she is relishing their new lease of life together. i know ill never have another valegro and this is probably the peak of my career, dujardin says. but who cares? there are thousands of masses that would like to have had this opportunity. ive been lucky to have it, i will always have it and i will enjoy it. it is a far cry from the rider that first turned up at the yard of hester, himself an olympic champion who described her as edwina scissorhands, because of her wooden riding skills akin to the character played by johnny depp in the 1990 movie edward scissorhands. hers and hesters relationship can still be a volatile one dujardin admitting we occasionally scream and shout at each other but it is clearly stronger for the certainty over valegros future. dujardin knows she can save the pairs last dance for some time.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df2.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9438d4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charlotte dujardin rode to double dressage atomic number 79 at the london olympics on valegro. simply the reverence was the partnership would embody separated with valegro up for sale. the partnership equal nowadays safe and the pair have gone on to break world records. dujardin now has the world deed of conveyance in her sights, the one major achiever missing from her cv.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df2.highlights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176ab42",
   "metadata": {},
   "source": [
    "#### Function definition for random deletion technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54bb4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_random_deletion(text):\n",
    "    # Create an augmentation object\n",
    "    aug = naw.RandomWordAug(action='delete')\n",
    "    # Augment the text\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d72942",
   "metadata": {},
   "source": [
    "#### calling the augmentation function and storing the results in augmented_test_df2 and then appending to the main train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "936c6ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/xqdd6m4n2p56hnvg1t8_81_c0000gn/T/ipykernel_8955/2320002192.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(augmented_train_df2, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "augmented_train_df2['article'] = twenty_percent_df2['article'].apply(augment_random_deletion)\n",
    "augmented_train_df2['highlights'] = twenty_percent_df2['highlights'].apply(augment_random_deletion)\n",
    "\n",
    "#appending the augmented rows to main cleaned train split df(test_df)from augmented_test_df2\n",
    "train_df = train_df.append(augmented_train_df2, ignore_index=True)\n",
    "\n",
    "# Save the augmented+train data to a CSV file for backup\n",
    "train_df.to_csv('augmented_train2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fe3425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc563661",
   "metadata": {},
   "source": [
    "#### After Random Deletion technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5ee67c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( cnn) sometimes the price of success is losing the thing you most cherish. for charlotte dujardin, every triumph in the sporting arena seemingly took her another footprint closer to an uncertain future. billed as the girl with the dancing horse after her heroics at the london 2012 olympics, she feared that every time she competed would be last waltz with her equine partner. the mere thought of losing her best friend reduced dujardin to tears. , the horse on which she won double gold, on which she was crowned double european champion and on which she aims to repeat the feat at augusts world championships, was attracting big offers. he was valued at 10 million but, after much speculation, valegros for sale sign has finally come down meaning they now have the chance to continue a remarkable recordsetting run together. his future is secure he is never going to be sold, we have him forever, the british rider says of the 12yearold horse, which is coowned by her mentor and fellow olympic champion carl hester and roly luard. its an unbelievable relief. it was really, really hard knowing that every competition we did might be our last one. i never knew what was going on, i was too afraid to ask, dujardin tells cnn. hester, who won team dressage gold alongside dujardin at london 2012, hopes to set up a small syndicate to cover the costs of keeping valegro in competition. there will be others i can sell on but this matchless is special, the 46yearold told horse and country earlier this year. i didnt want charlotte to feel like he might be sold its not great to ride under those conditions. besides, valegro has been with us since he was two so whoever we sell him to, are they going to look after him the way we do? probably not. dujardin describes valegro more affectionately known as blueberry at his yard as a ferrari and, to all intents and purposes, she is a fernando alonso as she wrestles the ultimate horsepower out of him. hes a real hot horse likes to work, and hes just the most willing and comfortable horse to ride, she says of their telepathic partnership. he trusts me implicitly, we both have that trust for each other. as for me, i just do what i do. i just have a feeling inside me and i cant wholly explain that feeling. dujardin is the world no. 1 in dressage, which is split into two upshot (the set routine and the freestyle) and holds the world records in all three elements of her play the freestyle, grand prix and grand prix special. her most recent record came at last months reem acra fei world cup final in lyon with a score of 87. 129 comfortably beating her previous milestone of 85. 942 set at the european championships. she scored 92. 179 in the freestyle section. surely there is a ceiling to the records? well, youd think that but he just gets better every time, dujardin says. i dont know what the limit is although i do believe we can at least beat the freestyle record. there still feels like theres more to come. dujardin is somewhat disbelieving at the success that she has achieved. it is true that she hasnt gone down the typical route. she did not have the privileged upbringing of many of her rivals. born in enfield, north london, she only got her chance when her grandmother joy died, leaving a fivefigure inheritance to give dujardin her the chance to buy her first horse of note. its odd when i look back, i have to pinch myself sometimes, i honestly cant believe it, she says. ive done more than i ever dreamed of. ive just worked hard to get to the top. ive had so many letters from people saying ive inspired them to take up riding and thats an amazing feeling. getting to the top is one thing but staying there is quite another. thats the difficult bit. dujardin will take a trip down memory lane on friday when competing at the royal windsor horse show, one of the first that she ever competed in as a child and a chance to her and valegros finery on home soil once more. her biggest home display will forever be the london 2012 olympics. the british team had entered it looking for a first olympic medal in the discipline in over a century, and she ended up picking up two golds) in the space of just a week. but she admits life after london was tough. whole experience was incredible. id just aimed to be there so that was it, id achieved my goal. to get those gold medals was just an unbelievable bonus. but after that died down i almost felt a bit depressed understandable after such a. it was difficult coming from doing and then little media to suddenly personify in the media spotlight. people wanted me to do everything, and i found that really, really tough. it was a very difficult time. part of that was down to the uncertainty of not knowing valegros future but obviously thats all changed. throughout it all, the pair kept on winning and breaking records, but the complete happiness has fully returned in 2014. in addition, the olympics has brought her all manner of other unlikely facets to her life, ranging from a friendship with charttopping singer leona lewis, who has come to see her train and compete, to passing on tips to former england footballer michael owen on dressage for his daughter, who also rides. none of that has fazed dujardin, who still bears the same trademark smile that delighted the crowds in london and has made her something of a smiling assassin in which she has decimated her rivals in competition. just the world title now is left on the todo list. should she win that, she says i will have achieved everything in my career at the age 28. itll be time to retire! but there are no retirement plans. she and valegro have plenty of competitions ahead of them. just she is relishing their new lease of life together. i know ill never have another valegro and this is probably the peak of my career, dujardin says. but who cares? there are thousands of masses that would like have had this opportunity. ive been lucky have it, i will always have it and i will enjoy it. it is a far cry from the rider that first turned up at the yard of hester, himself an olympic champion who described her as edwina scissorhands, because of her wooden riding skills akin to the character played by johnny depp in the 1990 movie edward scissorhands. hers and hesters relationship can still be a volatile one dujardin admitting we occasionally scream and shout at each other but it is clearly stronger for the certainty over valegros future. dujardin knows she can save the pairs last dance for some time.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#augmented_train_df2.shape\n",
    "augmented_train_df2.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e735102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charlotte dujardin rode to double dressage atomic number 79 the london olympics on valegro. simply the reverence the partnership would embody separated with valegro for sale. nowadays and pair have gone on to break world records. dujardin has the world deed of conveyance in sights, the one major achiever missing from her cv.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df2.highlights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071a4a0",
   "metadata": {},
   "source": [
    "### 3. Random Swap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921d5e9",
   "metadata": {},
   "source": [
    "#### taking a random 20% sample from train split for augmentation using 2nd technique (Random Swap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4e6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of data to augment\n",
    "augment_percentage = 0.2\n",
    "\n",
    "#new dataframe to store augmented data\n",
    "augmented_train_df3 = pd.DataFrame(columns=['article','highlights'])\n",
    "\n",
    "#dataframe to store 20% data taken from train split\n",
    "twenty_percent_df3 = pd.DataFrame()\n",
    "twenty_percent_df3 = train_df.sample(frac=augment_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ce08895",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_percent_df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bd3faa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'highlights'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fa845",
   "metadata": {},
   "source": [
    "#### Before applying Random Swap technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1e2f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by . daily mail reporter . published . 1746 est, 27 december 2013 . . updated . 1750 est, 27 december 2013 . the moment an armed 16yearold boy was shot dead by police after a highspeed chase in a stolen car and daylong manhunt has been caught on camera. peyton cole barbour was killed in a volley of gunfire in which two grand prairie police officers were injured in texas on christmas day . the mansfield lake ridge high school student was flown to hospital but died from his wounds. scroll down for video . tragic peyton barbour, 16, was shot dead by police on christmas day after leading them on a highspeed chase in a stolen car . troubled teen peyton cole barbour, 16, was a student at mansfield lake ridge high school in mansfield, texas . the shocking incident began about 4.30am wednesday when an officer tried to stop the vehicle the teen was driving near north grand peninsula drive and england parkway in grand prairie because of suspicious activity. barbour sped away in the stolen 1997 honda, leading officers on a highspeed pursuit. the armed teenager then dumped the car in a field, firing a stolen glock 9mm pistol at the officers as he ran into a wooded area, the dallas morning news reported. for about 10 hours, police officers, a k9 unit and a police helicopter searched the area from grand peninsula to lynn creek marina for barbour, nbc 5 reported. about 2pm, police officers swarmed the 600 block of seeton road after residents reported suspicious activity. thats when cops cornered barbour and the teen exchanged gunfire with officers. two cops suffered bullet wounds, but their injuries were not lifethreatening. barbour was struck by gunfire before being rushed to parkland memorial hospital where he died. cornered dramatic footage captures the moment peyton barbour was gunned down by police in an open field on christmas day . scene two police officers were wounded when peyton barbour opened fire in this open field . on the run barbour fled after dumping this stolen 1997 honda in a field . on camera witness tammy king captured the gunshots outside her home on her cell phone . witness tammy king captured the violent exchange on her cell phone. she said she locked eyes with the teen as he walked right by her driveway right before the shootout. i kind of got nervous, pulled in the drive and locked the door, she told khou.com. barbour’s father, oscar, is an assistant principal at nash elementary school in mansfield, according to the school’s website. the website also shows that his mother, dawn, works as a receptionist at cross timbers intermediate school. dawn barbour told nbc she and her husband saw their son on christmas morning, but there was no indication anything was wrong. this isn’t him, it was so out of character, didn’t see it coming, she said. i do want to say to the officers that were injured, im so sorry, i apologize, you know that he did this. we dont know, we really dont, there are so many questions it hit us like a ton of bricks, out of nowhere. police yesterday were still trying to determine how barbour obtained a glock 9mm pistol which been reported stolen in a home burglary. the boys school yesterday released a brief statement saying that officials were shocked and saddened by the news but thankful the wounded officers were recovering. barbour’s friends said the shootout was completely out of character because he never talked about guns or indicated he had one. trevor stutsman told khou.com barbour had made some stupid decisions in the past, but he was a great kid. family and friends laid flowers and personal notes on the hood of barbour’s red chevrolet iroc z camaro which was parked outside his house. mourning grieving friends and family left flowers on barbours red chevrolet iroc z camaro parked outside his house .'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df3.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b20094aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peyton cole barbour, 16, was shot dead by police on christmas day. the armed teen led cops on a chase in a stolen car in texas. two police officers were wounded. witness tammy king captured the shootout on her cell phone .'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_percent_df3.highlights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77226cf",
   "metadata": {},
   "source": [
    "#### Function definition for random swap technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad90f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_random_swap(text):\n",
    "    # Create an augmentation object\n",
    "    aug = naw.RandomWordAug(action='swap')\n",
    "    # Augment the text\n",
    "    #text = \"(cnn) -- with a breezy sweep of his pen president vladimir putin wrote a new chapter into crimea's turbulent history, committing the region to a future returned to russian domain. sixty years prior, ukraine's breakaway peninsula was signed away just as swiftly by soviet leader nikita khrushchev. but dealing with such a blatant land grab on its eastern flank will not be anywhere near as quick and easy for europe's 28-member union. because, unlike crimea's rushed referendum, everyone has a say. after initially slapping visa restrictions and asset freezes on a limited number of little known politicians and military men, europe is facing urgent calls to widen the scope of its measures to target the russian business community in particular. \"\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d6931",
   "metadata": {},
   "source": [
    "#### calling the augmentation function and storing the results in augmented_test_df3 and then appending to the main train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d8b3dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/xqdd6m4n2p56hnvg1t8_81_c0000gn/T/ipykernel_8955/537336346.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(augmented_train_df3, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "augmented_train_df3['article'] = twenty_percent_df3['article'].apply(augment_random_swap)\n",
    "augmented_train_df3['highlights'] = twenty_percent_df3['highlights'].apply(augment_random_swap)\n",
    "\n",
    "#appending the augmented rows to main cleaned train split df(test_df)from augmented_test_df3\n",
    "train_df = train_df.append(augmented_train_df3, ignore_index=True)\n",
    "\n",
    "# Save the augmented+train data to a CSV file for backup\n",
    "train_df.to_csv('augmented_train3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33e17bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_df3.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c48a71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'highlights'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a12491",
   "metadata": {},
   "source": [
    "#### After Random Swap technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac82b5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by. daily mail reporter. published. 1746 est, 27 december 2013. . updated. 1750 est, 27 december 2013. moment the an armed 16yearold boy was shot dead by police after a highspeed chase in stolen a car and daylong manhunt has been caught on camera. peyton cole barbour was killed in a volley of gunfire in which two grand prairie police officers were injured in texas on christmas day. the mansfield lake ridge high school student was flown to hospital but died from his wounds. scroll down for video. tragic peyton barbour, 16, was shot dead by police on christmas day after leading them on a highspeed chase in a stolen car. troubled teen peyton cole barbour, 16, was a student at mansfield lake ridge high school in mansfield, texas. the shocking incident began about 4. 30am wednesday when an officer tried to stop the vehicle the teen was driving near north grand peninsula drive and england parkway in grand prairie because of suspicious activity. barbour sped away in the stolen 1997 honda, leading officers on a highspeed pursuit. armed the teenager then dumped the car in a field, firing a stolen glock 9mm pistol at the officers as he ran into a wooded area, the dallas morning news reported. for about 10 hours, police officers, a k9 unit and a police helicopter searched area the from grand peninsula to lynn creek marina for barbour, nbc 5 reported. about 2pm, police officers swarmed the 600 block of seeton road after residents reported suspicious activity. thats when cops cornered barbour and the teen exchanged gunfire with officers. two cops suffered bullet wounds, but their injuries were not lifethreatening barbour. was struck by gunfire before being rushed to parkland memorial hospital where he died. cornered dramatic footage captures the moment peyton barbour was gunned down by police in an open field on christmas day. scene two police officers wounded were when peyton barbour opened fire in this open field. the on run barbour fled after dumping this stolen 1997 honda in a field. on camera witness tammy king captured the gunshots outside her home on her cell phone. witness tammy king captured the violent exchange on her cell phone. she said she locked eyes with the teen as he walked right by her driveway right before the shootout. i kind of got nervous, pulled in the drive and locked the door, she told khou. com. barbour ’ s father, oscar, is an assistant principal at nash elementary school in mansfield, according to the school ’ s website. the website also shows that his mother, dawn, works as a receptionist at cross timbers intermediate school. dawn barbour nbc told she and her husband saw their son on christmas morning, but there was no indication anything was wrong. this isn ’ t him, it was so out of character, didn ’ t see it coming, she said. i do want to say to the officers that were injured, im so sorry, i apologize, you know that he did this. we dont know, we really dont, there are so many questions it hit us like a ton of bricks, out of nowhere. police yesterday were still trying to determine how barbour obtained a glock 9mm pistol which been reported stolen in a home burglary. the boys school yesterday released a brief statement saying that officials were shocked and saddened by the news but thankful wounded the officers were recovering. barbour ’ s friends said the shootout was completely out of character because he never talked about guns or indicated he had one. trevor stutsman told khou com. barbour had made some stupid decisions in the past, but he was a great kid. family and friends laid flowers and personal notes on the hood of barbour ’ s red chevrolet iroc z camaro which was parked outside his house. mourning grieving friends and family left flowers on barbours red chevrolet iroc z camaro parked outside his house.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df3.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc8bb445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peyton, cole barbour 16 was, shot dead by police on christmas day. the armed teen cops led on chase a in a stolen car in texas. two police were officers wounded. witness king tammy captured the shootout on her phone cell.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_df3.highlights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74d9c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final csv file after all augmentation techniques is augmented_train3.csv and dataframe is ats_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d55733",
   "metadata": {},
   "source": [
    "#### Importing libraries and augmented train csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "701a6792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by . associated press . published . 1411 est, ...</td>\n",
       "      <td>bishop john folda, of north dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(cnn) with a breezy sweep of his pen president...</td>\n",
       "      <td>nina dos santos says europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fleetwood are the only team still to have a 10...</td>\n",
       "      <td>fleetwood top of league one after 20 win at sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hes been accused of making many a fashion faux...</td>\n",
       "      <td>prime minister and his family are enjoying an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by . daily mail reporter . published . 0115 es...</td>\n",
       "      <td>nba star calls for black and hispanic communit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  by . associated press . published . 1411 est, ...   \n",
       "1  (cnn) with a breezy sweep of his pen president...   \n",
       "2  fleetwood are the only team still to have a 10...   \n",
       "3  hes been accused of making many a fashion faux...   \n",
       "4  by . daily mail reporter . published . 0115 es...   \n",
       "\n",
       "                                          highlights  \n",
       "0  bishop john folda, of north dakota, is taking ...  \n",
       "1  nina dos santos says europe must be ready to a...  \n",
       "2  fleetwood top of league one after 20 win at sc...  \n",
       "3  prime minister and his family are enjoying an ...  \n",
       "4  nba star calls for black and hispanic communit...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"augmented_train3.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf27524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (480289, 2)\n",
      "Validation data:  (15442, 2)\n",
      "Test data:  (15441, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train data: ',data.shape)\n",
    "print('Validation data: ', val_df.shape)\n",
    "print('Test data: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc8692e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d4b5f",
   "metadata": {},
   "source": [
    "#### Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a034a605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from transformers import AutoTokenizer\\n\\ndf = data\\n\\n# Initialize the tokenizer\\ntokenizer = AutoTokenizer.from_pretrained('t5-base')\\n\\n# Tokenize the article and highlight texts and encode them as numerical representations\\ninputs = tokenizer(df['article'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\\ntargets = tokenizer(df['highlights'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import AutoTokenizer\n",
    "\n",
    "df = data\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Tokenize the article and highlight texts and encode them as numerical representations\n",
    "inputs = tokenizer(df['article'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\n",
    "targets = tokenizer(df['highlights'].tolist(), padding='max_length', truncation=True, return_tensors='pt')'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c94223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from transformers import AutoTokenizer\\n\\n#df = data.head(66707)\\n\\n# Initialize the tokenizer\\ntokenizer = AutoTokenizer.from_pretrained('t5-base')\\n\\n# Split the DataFrame into 8 batches\\ndf_batches = np.array_split(data, 8)\\n\\n# Initialize the tokenizer\\ntokenizer = AutoTokenizer.from_pretrained('t5-base')\\n\\n# Initialize an empty dictionary to store the tokenized data\\ndata_dict = {}\\n\\n# Iterate over the batches\\nfor i, batch in enumerate(df_batches):\\n    # Tokenize the text\\n    inputs = tokenizer(batch['article'].tolist(), padding=True, truncation=True, return_tensors='pt')\\n    targets = tokenizer(batch['highlights'].tolist(), padding=True, truncation=True, return_tensors='pt')\\n    \\n    # Combine the tokenized article and highlights into a single dictionary\\n    #batch_data = {input_seq: target_seq for input_seq, target_seq in zip(inputs['input_ids'], targets['input_ids'])}\\n    batch_data = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'decoder_input_ids': targets['input_ids'], 'decoder_attention_mask': targets['attention_mask']}\\n    #batch_data = {i: ats_data for i, ats_data in enumerate(zip(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'], targets['attention_mask']))}\\n\\n    \\n    # Add the batch data to the main dictionary\\n    data_dict.append(batch_data)\\n\\n# The resulting dictionary maps each input sequence to its corresponding target sequence\\nprint(data_dict)\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import AutoTokenizer\n",
    "\n",
    "#df = data.head(66707)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Split the DataFrame into 8 batches\n",
    "df_batches = np.array_split(data, 8)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Initialize an empty dictionary to store the tokenized data\n",
    "data_dict = {}\n",
    "\n",
    "# Iterate over the batches\n",
    "for i, batch in enumerate(df_batches):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(batch['article'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "    targets = tokenizer(batch['highlights'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Combine the tokenized article and highlights into a single dictionary\n",
    "    #batch_data = {input_seq: target_seq for input_seq, target_seq in zip(inputs['input_ids'], targets['input_ids'])}\n",
    "    batch_data = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'decoder_input_ids': targets['input_ids'], 'decoder_attention_mask': targets['attention_mask']}\n",
    "    #batch_data = {i: ats_data for i, ats_data in enumerate(zip(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'], targets['attention_mask']))}\n",
    "\n",
    "    \n",
    "    # Add the batch data to the main dictionary\n",
    "    data_dict.append(batch_data)\n",
    "\n",
    "# The resulting dictionary maps each input sequence to its corresponding target sequence\n",
    "print(data_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.article[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01974456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#Create a word tokenizer\n",
    "tokenizer = nltk.word_tokenize\n",
    "\n",
    "# Tokenize the article column in chunks\n",
    "for i in range(0, len(data), 1000):\n",
    "    data.loc[i:i+1000, \"article_tokens\"] = data.loc[i:i+1000, \"article\"].apply(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7732b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the highlights column in chunks\n",
    "for i in range(0, len(data), 1000):\n",
    "    data.loc[i:i+1000, \"highlights_tokens\"] = data.loc[i:i+1000, \"highlights\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c061f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480289, 4)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "957d1d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'highlights', 'article_tokens', 'highlights_tokens'], dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f18e470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article_tokens</th>\n",
       "      <th>highlights_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by . associated press . published . 1411 est, ...</td>\n",
       "      <td>bishop john folda, of north dakota, is taking ...</td>\n",
       "      <td>[by, ., associated, press, ., published, ., 14...</td>\n",
       "      <td>[bishop, john, folda, ,, of, north, dakota, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(cnn) with a breezy sweep of his pen president...</td>\n",
       "      <td>nina dos santos says europe must be ready to a...</td>\n",
       "      <td>[(, cnn, ), with, a, breezy, sweep, of, his, p...</td>\n",
       "      <td>[nina, dos, santos, says, europe, must, be, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  by . associated press . published . 1411 est, ...   \n",
       "1  (cnn) with a breezy sweep of his pen president...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  bishop john folda, of north dakota, is taking ...   \n",
       "1  nina dos santos says europe must be ready to a...   \n",
       "\n",
       "                                      article_tokens  \\\n",
       "0  [by, ., associated, press, ., published, ., 14...   \n",
       "1  [(, cnn, ), with, a, breezy, sweep, of, his, p...   \n",
       "\n",
       "                                   highlights_tokens  \n",
       "0  [bishop, john, folda, ,, of, north, dakota, ,,...  \n",
       "1  [nina, dos, santos, says, europe, must, be, re...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3c90e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39marticle_tokens[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.article_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a343b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "239bed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIAL STARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ca45b",
   "metadata": {},
   "source": [
    "# END OF FILE\n",
    "## Continued in other ipynb (after_tokenization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a27aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7343b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e2fe8949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a TfidfVectorizer object for the article_tokens column\n",
    "tfidf_article = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_article.fit(data['article_tokens'])\n",
    "\n",
    "# Fit and transform the article_tokens column\n",
    "tfidf_article_tokens = tfidf_article.transform(data['article_tokens'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a TfidfVectorizer object for the highlights_tokens column\n",
    "tfidf_highlights = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_highlights.fit(data['highlights_tokens'])\n",
    "\n",
    "# Fit and transform the highlights_tokens column\n",
    "tfidf_highlights_tokens = tfidf_highlights.transform(data['highlights_tokens'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de118a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "\n",
    "# Standardize the article_tokens column\n",
    "scaler_article = StandardScaler()\n",
    "scaled_article = []\n",
    "\n",
    "for i in range(0, len(tfidf_article_tokens.toarray()), batch_size):\n",
    "    batch = tfidf_article_tokens.toarray()[i:i+batch_size]\n",
    "    scaled_batch = scaler_article.fit_transform(batch)\n",
    "    scaled_article.append(scaled_batch)\n",
    "\n",
    "scaled_article = np.concatenate(scaled_article, axis=0)\n",
    "\n",
    "# Standardize the highlights_tokens column\n",
    "scaler_highlights = StandardScaler()\n",
    "scaled_highlights = []\n",
    "\n",
    "for i in range(0, len(tfidf_highlights_tokens.toarray()), batch_size):\n",
    "    batch = tfidf_highlights_tokens.toarray()[i:i+batch_size]\n",
    "    scaled_batch = scaler_highlights.fit_transform(batch)\n",
    "    scaled_highlights.append(scaled_batch)\n",
    "\n",
    "scaled_highlights = np.concatenate(scaled_highlights, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the article_tokens column\n",
    "scaler_article = StandardScaler()\n",
    "scaled_article = scaler_article.fit_transform(tfidf_article_tokens.toarray())\n",
    "\n",
    "# Standardize the highlights_tokens column\n",
    "scaler_highlights = StandardScaler()\n",
    "scaled_highlights = scaler_highlights.fit_transform(tfidf_highlights_tokens.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction using TruncatedSVD\n",
    "svd_article = TruncatedSVD(n_components=500, random_state=42)\n",
    "reduced_article = svd_article.fit_transform(scaled_article)\n",
    "\n",
    "# Perform dimensionality reduction using TruncatedSVD\n",
    "svd_highlights = TruncatedSVD(n_components=50, random_state=42)\n",
    "reduced_highlights = svd_highlights.fit_transform(scaled_highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the reduced_article and reduced_highlights in the dataframe\n",
    "data['reduced_article'] = list(reduced_article)\n",
    "data['reduced_highlights'] = list(reduced_highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90d060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62500542",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56496766",
   "metadata": {},
   "source": [
    "#### Unused cell. Do not run below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31efb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********Unused cell. Do not run this**************\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "'''from sklearn.model_selection import train_test_split\n",
    "input_ids_train, input_ids_val, attn_masks_train, attn_masks_val, decoder_input_train, decoder_input_val, decoder_mask_train, decoder_mask_val = train_test_split(\n",
    "    inputs['input_ids'], \n",
    "    inputs['attention_mask'], \n",
    "    targets['input_ids'], \n",
    "    targets['attention_mask'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the training and validation data into a single dictionary\n",
    "train_data = {'input_ids': input_ids_train, 'attention_mask': attn_masks_train, 'decoder_input_ids': decoder_input_train, 'decoder_attention_mask': decoder_mask_train}\n",
    "val_data = {'input_ids': input_ids_val, 'attention_mask': attn_masks_val, 'decoder_input_ids': decoder_input_val, 'decoder_attention_mask': decoder_mask_val}\n",
    "\n",
    "print(len(val_data))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac190c9",
   "metadata": {},
   "source": [
    "#### Combining tokenized article and highlights into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the input and target data into a single dictionary\n",
    "\n",
    "ats_data_new = {}\n",
    "ats_data_new = data_dict\n",
    "\n",
    "#ats_data_new = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'decoder_input_ids': targets['input_ids'], 'decoder_attention_mask': targets['attention_mask']}\n",
    "#print(ats_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cde974",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = ats_data_new\n",
    "\n",
    "# Convert the dictionary to a new dictionary with lists of values\n",
    "data_list = {k: [v.tolist() for v in sample_dict[k]] for k in sample_dict.keys()}\n",
    "\n",
    "# Create a DataFrame from the new dictionary\n",
    "sample_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe18374",
   "metadata": {},
   "source": [
    "#### Data Standardization using StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularize the data using standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "ats_data_new['input_ids'] = scaler.fit_transform(ats_data_new['input_ids'])\n",
    "#val_data['input_ids'] = scaler.transform(val_data['input_ids'])\n",
    "print(ats_data_new['input_ids'])\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4fbf9",
   "metadata": {},
   "source": [
    "#### Data Reduction using truncatedSVD (dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, reduce the dimensionality of the data using SVD as data reduction technique\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=256)\n",
    "ats_data_new['input_ids'] = svd.fit_transform(ats_data_new['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ats_data_new['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2cdb2",
   "metadata": {},
   "source": [
    "#### Converting the data into PyTorch tensors and creating a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors and create a DataLoader\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(ats_data_new['input_ids']), torch.tensor(ats_data_new['attention_mask']), torch.tensor(ats_data_new['decoder_input_ids']), torch.tensor(ats_data_new['decoder_attention_mask']))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f1e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702601de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict2 = ats_data_new\n",
    "\n",
    "# Convert the dictionary to a new dictionary with lists of values\n",
    "data_list2 = {k: [v.tolist() for v in sample_dict2[k]] for k in sample_dict2.keys()}\n",
    "\n",
    "# Create a DataFrame from the new dictionary\n",
    "sample_df2 = pd.DataFrame(data_list2)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(sample_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55588ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df2.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a6429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11549ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
